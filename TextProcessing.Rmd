---
title: "Text Processing"
author: "Matt Cliff"
date: "May 10, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if (!require(pacman)) install.packages('pacman')
pacman::p_load(sotu, ggplot2, tidytext, tidyverse, plyr, dplyr)
```

## Demo

This is going to be a demonstration to pull blocks of texts (political speeches) from various parties, then given a new block of text to identify if which poltical party it most associates with.

(Idea from a presentation given by Carlos Bossy who did this with state of the union addresses from two presidents.)



### References
Great [feature comparision](https://docs.quanteda.io/articles/pkgdown/comparison.html) from quanteda.

[Online Analysis](http://stateoftheunion.onetwothree.net/)

Possible places to soure
* [R sotu package](https://github.com/statsmaths/sotu)
* [https://en.wikisource.org/wiki/Portal:State_of_the_Union_Speeches_by_United_States_Presidents]
* [Data as Journalism](https://onlinejournalismblog.com/2018/02/05/text-as-data-journalism-sotu-speeches/)
* [Kaggle](https://www.kaggle.com/rtatman/state-of-the-union-corpus-1989-2017)

### Data

Using the *sotu* package the **sotu_meta** dataset is a dataframe with meta data of the 236 speeches, and sotu_text is a character vector with one address per element

```{r loaddata}


str(sotu_meta)
head(sotu_meta$year)
head(unlist(lapply(sotu_text, nchar)))
df <- data.frame(x=sotu_meta$year, y=unlist(lapply(sotu_text, nchar)))
head(df)
ggplot(data=df, aes(x, y)) + geom_point()


ggplot(sotu_meta, aes(x=party, fill=sotu_type)) + 
    geom_bar() +
    coord_flip()

```


best case, some nested folder structure based on political party
what kind of meta data about the text





Let's call on tidy text next
```{r tidy1}

df <- mutate(sotu_meta, text=sotu_text)
str(sotu_text[236])
sotu_meta[236,]

tokens <- data.frame(text=sotu_text[236], stringsAsFactors=FALSE) %>% unnest_tokens(word, text)


str(tokens)
head(tokens)
head(tokens %>% inner_join(get_sentiments("bing")))

getsentiment <- function(textdata) {
    #print(class(data))
    #print(str(data))
    sentiment <- data.frame(text=textdata, stringsAsFactors = FALSE) %>%
        unnest_tokens(word, text) %>%
        inner_join(get_sentiments("bing")) %>%
        dplyr::count(sentiment) %>%  # count the number of positive and negatives
        spread(sentiment, n, fill=0)  %>%  # made data wider rather than narrow
        mutate(sentiment = positive - negative)


        
    return(sentiment)
}


#    %>%
#        mutate(president = data$president) %>%
#        mutate(sotu_type = data$sotu_type) %>%


df <- ldply(lapply(sotu_text, getsentiment), data.frame)

head(df)
head(cbind(sotu_meta, df))
a <- cbind(sotu_meta, df)
str(a)
class(a)

#sentiments <- getsentiment(df)


ggplot(a, aes(x = as.numeric(year), y = sentiment)) + 
  geom_point() + # add points to our plot, color-coded by president
  geom_smooth(method = "auto") # pick a method & fit a model
    

  #geom_point(aes(color = president)) + # add points to our plot, 

```





### Model
Use TF_IDF *Term Frequency - Inverse Document Frequency* to score the words,  then we will have set of matrixes for each category, we build our model to classify on the matrix of input text





### Results

Goals would be to able to do this in different countries (start with England, Australia, Canada)

How does this change over time?  If I train with text from now, to compare with 30 years ago?

What about twitter? Train with twitter, how does speeches match?
Can we match to a person?