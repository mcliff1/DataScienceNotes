---
title: "BasicModelAnalysis"
author: "Matt Cliff"
date: "January 29, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(caret)
require(caretEnsemble)
require(dplyr)
```

## Basic Model Analysis

My goal is to set up a page that I can inject a dataset into
then run numerous models to exhaustion to identify to best one or ones to tune.

To begin with I will need to limit the problem domain based on regression or classification methods, although the underlying mechanics is almost always linear algebra the approaches to these problems can diverge significantly.

In this case we will focus on the Parkison's dataset with 197 observations, and 23 predictors of a binary outcome.

### Classification Problem

The Classification (value) problem is defined as one where a particular observation of $n$ predictors will have a single valued dependent (predicted) variable that is from a finite class.  The predictor or indepdenent variables can be either finite valued or real valued (or vector valued).

Our goal is to come up with some model function $f: \mathbb{X}_1 \times \mathbb{X}_2\times \dots \times \mathbb{X}_n\mapsto \mathbb{Y}$ that matches our data and we think can best be used to predict the dependent variable for future observations from the domain space.

Each of the $\mathbb{X_j}$ could be discrete or continuous, we will not have any practicle restriction except that they MUST be valued, any missing data must be dealt with prior to using this package.

We want to find something with at least 10 features, and 5000 observations
[UCI Datasets](https://archive.ics.uci.edu/ml/datasets.html?format=&task=cla&att=&area=&numAtt=10to100&numIns=&type=&sort=instDown&view=table)

* [Buzz in Social Media](https://archive.ics.uci.edu/ml/datasets/Buzz+in+social+media+)

* [Sensorless drive](https://archive.ics.uci.edu/ml/datasets/Dataset+for+Sensorless+Drive+Diagnosis) - 49 attributes, 11 classes, 58000 records

* [card payment](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients) binary classifier with 23 predictors

* [parkinsons](https://archive.ics.uci.edu/ml/datasets/Parkinsons) binary classifier with 23 predictors; 197 instances

```{r loaddata}
load("data/ccData.RData")
load("data/pdData.RData")
#data("possum", package="DAAG")

```

The basic approach will be to ensure no zero elements
NAndx <- sapply(data, function(x) mean(is.na(x))) > .9
nzv <- nearZeroVar(data)



## Parkinsons

Let's take a closer look at the data

```{r plot}
pdFit <- train(status ~ ., data=pdData)
plot(varImp(pdFit, varImp.train=FALSE), top=10)
plot(pdFit)
plot(pdFit$finalModel)
pdFit$finalModel
```

Now let's pull out a training and test set and see what kind of predictions we can make with this

```{r pkModel, echo=FALSE}
inTrain <- createDataPartition(pdData$status, p=0.8, list=FALSE)
training <- pdData[inTrain,]
testing <- pdData[-inTrain,]
control <- trainControl(method="cv", number=4, allowParallel=TRUE)

#preproc <- preProcess(select(training,-(status)), method=c("center", "scale", "pca"), thresh = 0.8)
preproc <- preProcess(select(training,-(status)), method="pca", thresh = 0.8)

# metric <- "Accuracy"
```
Experiment with different models

```{r pkModel2}
#formula.all <- as.forumula(paste("status ~ ", paste(features, collapse="+")))

set.seed(10)
fit.rf <- train(status ~ ., data=training, method="rf", trControl=control)

set.seed(10)
fit.knn <- train(status ~ ., data=training, method="knn", trControl = control)

set.seed(10)
fit.glm <- train(status ~ ., data=training, method="glm", preProcess="pca", trControl = control)

results <- resamples(list(rf=fit.rf, knn=fit.knn, glm=fit.glm))
summary(results)



```

check some predictions

```{r pkPredict}
predict.rf <- predict(fit.rf, newdata=testing)
confMat.rf <- confusionMatrix(predict.rf, testing$status)

predict.knn <- predict(fit.knn, newdata=testing)
confMat.knn <- confusionMatrix(predict.knn, testing$status)

predict.glm <- predict(fit.glm, newdata=testing)
confMat.glm <- confusionMatrix(predict.glm, testing$status)

confMat.rf$overall
confMat.knn$overall
confMat.glm$overall

performance <- matrix(round(c(confMat.rf$overall, confMat.knn$overall, confMat.glm$overall), 3), ncol=3)
colnames(performance) <- c('Random Forest', 'K-Nearest Neighbors', 'GLM pca')
print(as.table(performance))
```

Lastly try using caretEnsamble
```{r caretEnsamble}
xgbTreeGrid <- expand.grid(nrounds=400, max_depth=3, eta=0.1, 
                           gamma = 0, 
                           colsample_bytree = 1.0, 
                           subsample = 1.0, 
                           min_child_weight = 4)

glmGrid <- expand.grid(.alpha = 1, .lambda=seq(0.001,0.1, by=0.001))

#modelList <- caretList(
#    status ~ ., data=training, trControl=control,
#    tuneList = list(
#        xgbTree = caretModelSpec(method="xgbTree", tuneGrid = xgbTreeGrid, nthread=8),
#        glmnet = caretModelSpec(method="glm", tuneGrid=glmGrid)
#    )
#)
#xyplot(resamples(modelList))

```
Fina